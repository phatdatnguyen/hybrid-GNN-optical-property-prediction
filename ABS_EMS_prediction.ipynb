{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1aa19ac",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008947a4-481c-44aa-9c48-fa576373167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator, rdmolops\n",
    "from rdkit.Avalon import pyAvalonTools\n",
    "import deepchem as dc\n",
    "import torch\n",
    "from torch.nn import Linear, BatchNorm1d, ModuleList\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import TransformerConv, global_mean_pool, summary\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411fd119-207f-4dbd-af92-6e101831c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random_seed = 0\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28335743",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromophoresDataset(Dataset):\n",
    "    def __init__(self, data_file_path, dataset_name, target_column, gcn_featurizer_name, mol_featurizer_name, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.data_file_name = os.path.basename(data_file_path)        \n",
    "        self.target_column = target_column\n",
    "        self.output_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.gcn_featurizer_name = gcn_featurizer_name\n",
    "        self.mol_featurizer_name = mol_featurizer_name\n",
    "        self.mol_features_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        os.makedirs('./Datasets', exist_ok=True)\n",
    "        root = './Datasets/' + dataset_name # Where the dataset should be stored. This folder is split into 'raw' and 'processed'.\n",
    "        os.makedirs(root, exist_ok=True)\n",
    "        os.makedirs(root + '/raw', exist_ok=True)\n",
    "        shutil.copy2(data_file_path, root + '/raw')\n",
    "        super(ChromophoresDataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self): # If this file exists in 'raw', the download is not triggered.\n",
    "        return self.data_file_name\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
    "        return [f'data_{i}.pt' for i in list(self.data.index)]\n",
    "    \n",
    "    def process(self):\n",
    "        # Read the data file and fit the output scaler\n",
    "        self.data = pd.read_csv('./Datasets/' + self.dataset_name + '/raw/' + self.data_file_name)\n",
    "        self.output_scaler.fit(self.data[self.target_column].to_numpy().reshape(-1, 1))\n",
    "\n",
    "        if len(os.listdir(self.processed_dir)) > 2: # If these files are found in 'processed', processing is skipped.\n",
    "            mol_features_list = np.loadtxt('Datasets/' + self.dataset_name + '/raw/mol_features.csv', delimiter=',')\n",
    "            self.mol_features_scaler.fit(np.array(mol_features_list))\n",
    "\n",
    "            return\n",
    "        \n",
    "        # Initialize the featurizers\n",
    "        if self.gcn_featurizer_name == 'MGCF':\n",
    "            gcn_featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True, use_chirality=True, use_partial_charge=True)\n",
    "        elif self.gcn_featurizer_name == 'PMGF':\n",
    "            gcn_featurizer = dc.feat.PagtnMolGraphFeaturizer(max_length=5)\n",
    "        else: #self.gcn_featurizer_name == 'DMPNNF'\n",
    "            gcn_featurizer = dc.feat.DMPNNFeaturizer(is_adding_hs=False)\n",
    "        \n",
    "        if (self.mol_featurizer_name == 'Mordred'):\n",
    "            mol_featurizer = dc.feat.MordredDescriptors(ignore_3D=True)\n",
    "        elif (self.mol_featurizer_name == 'MorganFP'):\n",
    "            mol_featurizer = dc.feat.CircularFingerprint(size=2048, radius=3)\n",
    "        else:\n",
    "            mol_featurizer = None # for other RDKit's fingerprints\n",
    "\n",
    "        mol_features_list = []\n",
    "        graph_index = 0\n",
    "        for index, row in tqdm(self.data.iterrows(), total=self.data.shape[0]):\n",
    "            try:\n",
    "                # Graph featurization for chromophores\n",
    "                chromophore_mol_obj = Chem.MolFromSmiles(row['Chromophore'])\n",
    "                chromophore_mol_obj = Chem.AddHs(chromophore_mol_obj)\n",
    "                gcn_features = gcn_featurizer.featurize(chromophore_mol_obj)\n",
    "                node_feats = torch.tensor(np.array(gcn_features[0].node_features))\n",
    "                self.n_gcn_features = node_feats.shape[1]\n",
    "                edge_index = torch.tensor(np.array(gcn_features[0].edge_index))\n",
    "                edge_attr = torch.tensor(np.array(gcn_features[0].edge_features))\n",
    "\n",
    "                # Molecular fingerprint featurization for chromophores\n",
    "                if mol_featurizer is not None: # Deepchem's Mordred descriptors or Morgan fingerprint\n",
    "                    mol_features = mol_featurizer.featurize(row['Chromophore'])\n",
    "                else: # RDKit's fingerprints\n",
    "                    if self.mol_featurizer_name == 'AvalonFP':\n",
    "                        mol_features = np.array(pyAvalonTools.GetAvalonFP(chromophore_mol_obj, nBits=2048)).reshape(1, -1)\n",
    "                    elif self.mol_featurizer_name == 'APFP':\n",
    "                        mol_features = rdFingerprintGenerator.GetAtomPairGenerator(fpSize=2048).GetFingerprintAsNumPy(chromophore_mol_obj).reshape(1, -1)\n",
    "                    elif self.mol_featurizer_name == 'TTFP':\n",
    "                        mol_features = rdFingerprintGenerator.GetTopologicalTorsionGenerator(fpSize=2048).GetFingerprintAsNumPy(chromophore_mol_obj).reshape(1, -1)\n",
    "                    elif self.mol_featurizer_name == 'LayeredFP':\n",
    "                        mol_features = np.array(rdmolops.LayeredFingerprint(chromophore_mol_obj, fpSize=2048)).reshape(1, -1)\n",
    "                    elif self.mol_featurizer_name == 'PatternFP':\n",
    "                        mol_features = np.array(rdmolops.LayeredFingerprint(chromophore_mol_obj, fpSize=2048)).reshape(1, -1)\n",
    "                    else: # self.mol_featurizer_name == 'RDKitFP':\n",
    "                        mol_features = rdFingerprintGenerator.GetRDKitFPGenerator(fpSize=2048).GetFingerprintAsNumPy(chromophore_mol_obj).reshape(1, -1)\n",
    "                mol_features_list.append(mol_features.reshape(-1))\n",
    "                mol_features = torch.tensor(mol_features)\n",
    "                self.n_mol_features = mol_features.shape[1]\n",
    "\n",
    "                # Target\n",
    "                output = self.get_output(row[self.target_column])\n",
    "\n",
    "                # Save data to drive\n",
    "                data = Data(x=node_feats, \n",
    "                            edge_index=edge_index,\n",
    "                            edge_attr=edge_attr,\n",
    "                            y=output,\n",
    "                            smiles=row['Chromophore'],\n",
    "                            solvent_smiles=row['Solvent'],\n",
    "                            mol_features=mol_features\n",
    "                            )\n",
    "                \n",
    "                torch.save(data, os.path.join(self.processed_dir, \n",
    "                                 f'{self.dataset_name}_{graph_index}.pt'))\n",
    "                \n",
    "                graph_index += 1\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        # Save the mol_features list to a CSV file and fit the feature scaler\n",
    "        mol_features_arr = np.array(mol_features_list)\n",
    "        np.savetxt('Datasets/' + self.dataset_name + '/raw/mol_features.csv', mol_features_arr, delimiter=',')\n",
    "        self.mol_features_scaler.fit(mol_features_arr)\n",
    "\n",
    "    def get_output(self, output):\n",
    "        output = self.output_scaler.transform(np.array([output]).reshape(-1, 1))\n",
    "        return torch.tensor(output)\n",
    "\n",
    "    def len(self):\n",
    "        _, _, files = next(os.walk('./Datasets/' + self.dataset_name + '/processed'))\n",
    "        return len(files) - 2\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(os.path.join(self.processed_dir, f'{self.dataset_name}_{idx}.pt'), weights_only=False)   \n",
    "        return data\n",
    "    \n",
    "    @property\n",
    "    def num_node_features(self):\n",
    "        if (self.gcn_featurizer_name == 'MGCF'):\n",
    "            return 33\n",
    "        elif (self.gcn_featurizer_name == 'PMGF'):\n",
    "            return 94\n",
    "        elif (self.gcn_featurizer_name == 'DMPNNF'):\n",
    "            return 133\n",
    "        \n",
    "    @property\n",
    "    def num_edge_features(self):\n",
    "        if (self.gcn_featurizer_name == 'MGCF'):\n",
    "            return 11\n",
    "        elif (self.gcn_featurizer_name == 'PMGF'):\n",
    "            return 42\n",
    "        elif (self.gcn_featurizer_name == 'DMPNNF'):\n",
    "            return 14\n",
    "    \n",
    "    @property\n",
    "    def num_mol_features(self):\n",
    "        if (self.mol_featurizer_name == 'Mordred'):\n",
    "            return 1613\n",
    "        elif (self.mol_featurizer_name == 'MorganFP'):\n",
    "            return 2048\n",
    "        elif (self.mol_featurizer_name == 'AvalonFP'):\n",
    "            return 2048\n",
    "        elif (self.mol_featurizer_name == 'APFP'):\n",
    "            return 2048\n",
    "        elif (self.mol_featurizer_name == 'TTFP'):\n",
    "            return 2048\n",
    "        elif (self.mol_featurizer_name == 'LayeredFP'):\n",
    "            return 2048\n",
    "        elif (self.mol_featurizer_name == 'PatternFP'):\n",
    "            return 2048\n",
    "        elif (self.mol_featurizer_name == 'RDKitFP'):\n",
    "            return 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de6e3cc",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset file\n",
    "data_file_path = './Datasets/Abs_unique.csv'   # Options: 'Abs_unique.csv', 'Abs_with_solvent.csv', 'Ems_unique.csv', 'Ems_with_solvent.csv'\n",
    "\n",
    "# Choose molecular fingerprint and graph featurizer\n",
    "gcn_featurizer_name = 'MGCF'  # Options: 'MGCF', 'PMGF', 'DMPNNF'\n",
    "mol_featurizer_name = 'AvalonFP'  # Options: 'Mordred', 'MorganFP', 'AvalonFP', 'APFP', 'TTFP', 'LayeredFP', 'PatternFP', 'RDKitFP'\n",
    "\n",
    "# Define the dataset name and target column\n",
    "target_column = 'ABS'   # Options: 'ABS', 'EMS'\n",
    "chromophores_dataset_name = f'Abs_unique_{gcn_featurizer_name}_{mol_featurizer_name}'\n",
    "\n",
    "# Cownload and process the dataset of chromophores\n",
    "chromophores_dataset = ChromophoresDataset(data_file_path, chromophores_dataset_name, target_column, gcn_featurizer_name, mol_featurizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceb46d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation-test splitting\n",
    "train_size = int(len(chromophores_dataset) * 0.6)\n",
    "val_size = int(len(chromophores_dataset) * 0.2)\n",
    "test_size = len(chromophores_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(chromophores_dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54416562",
   "metadata": {},
   "source": [
    "# Hybrid GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN model definition\n",
    "class HybridGNNModel(torch.nn.Module):\n",
    "    def __init__(self, n_gcn_inputs, n_gcn_hiddens, n_gcn_layers, n_gcn_heads, n_gcn_outputs, edge_dim,\n",
    "                 n_mlp_inputs, n_mlp_hiddens, n_mlp_layers, n_mlp_outputs,\n",
    "                 n_predictor_hiddens, n_predictor_layers):\n",
    "        super(HybridGNNModel, self).__init__()\n",
    "        \n",
    "        # Check output size\n",
    "        if n_gcn_outputs == 0 and n_mlp_outputs == 0:\n",
    "            raise Exception(\"The total output size of GCN and MLP modules cannot be 0!\")\n",
    "        \n",
    "        # GCN layers (can be modified to use different GCN architectures)\n",
    "        self.gcn = torch.nn.ModuleList()\n",
    "        if n_gcn_outputs > 0:\n",
    "            self.gcn.append(TransformerConv(n_gcn_inputs, n_gcn_hiddens, n_gcn_heads, edge_dim=edge_dim))\n",
    "            self.gcn.append(Linear(n_gcn_hiddens*n_gcn_heads, n_gcn_hiddens))\n",
    "            self.gcn.append(BatchNorm1d(n_gcn_hiddens))\n",
    "            for i in range(1, n_gcn_layers):\n",
    "                self.gcn.append(TransformerConv(n_gcn_hiddens, n_gcn_hiddens, n_gcn_heads, edge_dim=edge_dim))\n",
    "                if i != n_gcn_layers - 1:\n",
    "                    self.gcn.append(Linear(n_gcn_hiddens*n_gcn_heads, n_gcn_hiddens))\n",
    "                    self.gcn.append(BatchNorm1d(n_gcn_hiddens))\n",
    "                else:\n",
    "                    self.gcn.append(Linear(n_gcn_hiddens*n_gcn_heads, n_gcn_outputs))\n",
    "                    self.gcn.append(BatchNorm1d(n_gcn_outputs))\n",
    "        \n",
    "        # MLP layers\n",
    "        self.mlp = ModuleList()\n",
    "        if n_mlp_outputs > 0:\n",
    "            self.mlp.append(Linear(n_mlp_inputs, n_mlp_hiddens))\n",
    "            for i in range(1, n_mlp_layers):\n",
    "                self.mlp.append(Linear(n_mlp_hiddens, n_mlp_hiddens))\n",
    "            self.mlp.append(Linear(n_mlp_hiddens, n_mlp_outputs))\n",
    "        \n",
    "        # Predictor layers\n",
    "        self.predictor = ModuleList()\n",
    "        if n_predictor_layers > 0:\n",
    "            self.predictor.append(Linear(n_gcn_outputs + n_mlp_outputs, n_predictor_hiddens))\n",
    "            for i in range(1, n_predictor_layers):\n",
    "                self.predictor.append(Linear(n_predictor_hiddens, n_predictor_hiddens))\n",
    "        \n",
    "        if n_predictor_layers > 0:\n",
    "            self.out = Linear(n_predictor_hiddens, 1)\n",
    "        else:\n",
    "            self.out = Linear(n_gcn_outputs + n_mlp_outputs, 1)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr, batch_index, mol_features):\n",
    "        if len(self.gcn) > 0:\n",
    "            for i, layer in enumerate(self.gcn):\n",
    "                if i == 0:\n",
    "                    h1 = layer(x, edge_index, edge_attr)\n",
    "                elif i % 3 == 0: # GCN layer\n",
    "                    h1 = layer(h1, edge_index, edge_attr)\n",
    "                elif i % 3 == 1: # Linear layer\n",
    "                    h1 = torch.relu(layer(h1)) \n",
    "                else: # BatchNorm1d layer\n",
    "                    h1 = layer(h1)\n",
    "            h1 = global_mean_pool(h1, batch_index)\n",
    "        else:\n",
    "            h1 = None\n",
    "\n",
    "        if len(self.mlp) > 0:\n",
    "            h2 = mol_features\n",
    "            for i, linear in enumerate(self.mlp):\n",
    "                h2 = torch.relu(linear(h2))\n",
    "        else:\n",
    "            h2 = None\n",
    "\n",
    "        if h1 != None and h2 != None:\n",
    "            h = torch.cat((h1, h2), dim=1)\n",
    "        elif h2 != None:\n",
    "            h = h2\n",
    "        elif h1 != None:\n",
    "            h = h1\n",
    "        \n",
    "        if len(self.predictor) > 0:\n",
    "            for i, linear in enumerate(self.predictor):\n",
    "                h = torch.relu(linear(h))\n",
    "        \n",
    "        return self.out(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a481a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with the parameters\n",
    "gcn_n_inputs = chromophores_dataset.num_node_features\n",
    "gcn_n_hiddens = 128\n",
    "gcn_n_layers = 3\n",
    "gcn_n_heads = 3\n",
    "gcn_n_outputs = 50\n",
    "edge_dim = chromophores_dataset.num_edge_features\n",
    "mlp_n_inputs = chromophores_dataset.num_mol_features\n",
    "mlp_n_hiddens = 128\n",
    "mlp_n_layers = 3\n",
    "mlp_n_outputs = 50\n",
    "predictor_n_hiddens = 128\n",
    "predictor_n_layers = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = HybridGNNModel(gcn_n_inputs, gcn_n_hiddens, gcn_n_layers, gcn_n_heads, gcn_n_outputs, edge_dim,\n",
    "                            mlp_n_inputs, mlp_n_hiddens, mlp_n_layers, mlp_n_outputs,\n",
    "                            predictor_n_hiddens, predictor_n_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec42ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the molecular fingerprint and fit the scaler\n",
    "mol_features_arr = np.loadtxt('./Datasets/' + chromophores_dataset.dataset_name + '/raw/mol_features.csv', delimiter=\",\")\n",
    "mol_features_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "mol_features_scaler.fit(mol_features_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf912c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "graph_data = chromophores_dataset[0]\n",
    "x = graph_data.x.float().to(device)\n",
    "edge_index = graph_data.edge_index.long().to(device)\n",
    "edge_attr = graph_data.edge_attr.float().to(device)\n",
    "batch_index = torch.tensor([0]).to(device)\n",
    "mol_features = graph_data.mol_features.numpy()\n",
    "mol_features_scaled = mol_features_scaler.transform(mol_features)\n",
    "mol_features_scaled = torch.tensor(mol_features_scaled).float().to(device)\n",
    "\n",
    "print(summary(model, x, edge_index, edge_attr, batch_index, mol_features_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876cadf3",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe5506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer, scheduler, and loss function\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "trained_epochs = 0\n",
    "optimizer = optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae03d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_index, batch in enumerate(dataloader, 1):\n",
    "        batch = batch.to(device)\n",
    "        x = batch.x.float()\n",
    "        edge_index = batch.edge_index.long()\n",
    "        edge_attr = batch.edge_attr.float()\n",
    "        mol_scaled = torch.tensor(\n",
    "            mol_features_scaler.transform(batch.mol_features.cpu()),\n",
    "            dtype=torch.float32, device=device\n",
    "        )\n",
    "        y = batch.y.float()\n",
    "        output = model(x, edge_index, edge_attr, batch.batch, mol_scaled)\n",
    "        loss = loss_fn(output, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += torch.sqrt(loss).item()\n",
    "    return total_loss / batch_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb01ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function\n",
    "@torch.no_grad()\n",
    "def validation(dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch_index, batch in enumerate(dataloader, 1):\n",
    "        batch = batch.to(device)\n",
    "        x = batch.x.float()\n",
    "        edge_index = batch.edge_index.long()\n",
    "        edge_attr = batch.edge_attr.float()\n",
    "        mol_scaled = torch.tensor(\n",
    "            mol_features_scaler.transform(batch.mol_features.cpu()),\n",
    "            dtype=torch.float32, device=device\n",
    "        )\n",
    "        y = batch.y.float()\n",
    "        output = model(x, edge_index, edge_attr, batch.batch, mol_scaled)\n",
    "        loss = loss_fn(output, y)\n",
    "        total_loss += torch.sqrt(loss).item()\n",
    "    return total_loss / batch_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c0f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 100\n",
    "t = tqdm(range(trained_epochs+1, epochs+trained_epochs+1), total=epochs, desc=\"Training\")\n",
    "for epoch in t:\n",
    "    train_loss = train(train_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    val_loss = validation(val_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    scheduler.step()\n",
    "    trained_epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss\n",
    "figure = plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('BCE with logits loss')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de53b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "@torch.no_grad()\n",
    "def test(dataloader):\n",
    "    model.eval()\n",
    "    y_test, y_pred, smiles_arr, solvent_smiles_arr = [], [], [], []\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        x = batch.x.float()\n",
    "        edge_index = batch.edge_index.long()\n",
    "        edge_attr = batch.edge_attr.float()\n",
    "        mol_scaled = torch.tensor(\n",
    "            mol_features_scaler.transform(batch.mol_features.cpu()),\n",
    "            dtype=torch.float32, device=device\n",
    "        )\n",
    "        y = batch.y.float()\n",
    "        output = model(x, edge_index, edge_attr, batch.batch, mol_scaled)\n",
    "        y_test.append(y.cpu())\n",
    "        y_pred.append(output.cpu())\n",
    "        smiles_arr.extend(batch.smiles)\n",
    "        solvent_smiles_arr.extend(batch.solvent_smiles)\n",
    "    y_test = torch.cat(y_test, dim=0)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    return y_test, y_pred, smiles_arr, solvent_smiles_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c24aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the output scaler\n",
    "output_scaler = chromophores_dataset.output_scaler\n",
    "\n",
    "# Run the model forward to get the test result\n",
    "global test_smiles_arr\n",
    "y_test_scaled, y_pred_scaled, test_smiles_arr, test_solvent_smiles_arr = test(test_dataloader)\n",
    "\n",
    "# Scale the outputs back to the original range\n",
    "global y_test\n",
    "global y_pred\n",
    "y_test = output_scaler.inverse_transform(y_test_scaled.reshape(-1,1)).reshape(-1)\n",
    "y_pred = output_scaler.inverse_transform(y_pred_scaled.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "# Evaluate the model using different metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse**0.5\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'MAE: {mae}\\nMSE: {mse}\\nRMSE: {rmse}\\nR2: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998a68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "scatter_plot = plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('actual value')\n",
    "plt.ylabel('predicted value')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cddb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "def save_checkpoint(model_name):\n",
    "    checkpoint = {\n",
    "        'state_dict': model.state_dict()\n",
    "    }\n",
    "    checkpoint_dir = './Checkpoints'\n",
    "    checkpoint_name = f'{model_name}_{trained_epochs}.ckpt'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    torch.save(checkpoint, checkpoint_dir + '/' + checkpoint_name)\n",
    "    return f'Model was saved to {checkpoint_dir}/{checkpoint_name}.'\n",
    "\n",
    "save_checkpoint(chromophores_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b5540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint\n",
    "def load_checkpoint(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    global train_losses\n",
    "    global val_losses\n",
    "    global trained_epochs\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    trained_epochs = 0\n",
    "\n",
    "    return f'Checkpoint loaded: {os.path.basename(checkpoint_path)}.'\n",
    "\n",
    "load_checkpoint(f'{chromophores_dataset_name}_10.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export losses\n",
    "df = pd.DataFrame()\n",
    "df['train_losses'] = train_losses\n",
    "df['val_losses'] = val_losses\n",
    "file_path = f'./{chromophores_dataset.dataset_name}_{trained_epochs}_losses.csv'\n",
    "df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a45744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export scatter plot\n",
    "df = pd.DataFrame()\n",
    "df['y_test'] = y_test.tolist()\n",
    "df['y_pred'] = y_pred.tolist()\n",
    "df['Chromophore'] = test_smiles_arr\n",
    "df['Solvent'] = test_solvent_smiles_arr\n",
    "file_path = f'./{chromophores_dataset.dataset_name}_{trained_epochs}_eval.csv'\n",
    "df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e1a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
